{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and problem statement: \"https://archive.ics.uci.edu/ml/datasets/Statlog+%28Shuttle%29\" <br>\n",
    "Code obtained from: <br>\n",
    "\n",
    "Title: \"Neural Networks from Scratch in Python\" <br>\n",
    "Authors: Harrison Kinsley & Daniel Kukie≈Ça <br>\n",
    "Publisher: Harrison Kinsley, 2020"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_FC: # Dense/fully connected\n",
    "\n",
    "    def __init__(self,num_inputs,num_neurons):\n",
    "        self.weights = 0.01*np.random.randn(num_inputs,num_neurons)\n",
    "        self.biases = np.zeros((1,num_neurons))\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions - Rectilinear and Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectilinear activation function\n",
    "class Activation_ReLU:\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation for classification\n",
    "class Activation_Softmax:\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        # We take the subtraction of the max of the inputs to avoid \"exploding values\" and \"dead neurons\"\n",
    "        exps = np.exp(inputs - np.max(inputs,axis=1,keepdims=True)) \n",
    "        probabilities = exps/np.sum(exps,axis=1,keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        # Uninitialised array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index,(single_output,single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
    "            # Flatten output array \n",
    "            single_output = single_output.reshape(-1,1)  # coloumn matrix\n",
    "            jacobian = np.diagflat(single_output) - np.dot(single_output,single_output.T)\n",
    "            # Calculate the sample-wise gradient and add to array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian,single_dvalues)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def calculate(self,output,y):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss): # Inherits Loss\n",
    "\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples = len(y_pred)\n",
    "        # CLip to prevent division by zero and clip both sides to drage mean to any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped* y_true,\n",
    "                axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_probabs = -np.log(correct_confidences)\n",
    "        return negative_log_probabs\n",
    "    \n",
    "    def backward(self, dvalues, y_true): ### We gonna only use one sample... use integer encoding instead\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample, we use the first one to count ????\n",
    "        lables = len(dvalues[0])\n",
    "        # If lables are sparse, turn them into one-hot vector??\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(lables)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true/dvalues\n",
    "        # Normalise gradient\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "\n",
    "class Activation_Softmax_Loss_CategoryCrossEntropy():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self,inputs,y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output ### ??????????????????\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true): ### We gonna only use one sample... use integer encoding instead\n",
    "        samples = len(dvalues)\n",
    "        # If lables are one-hot encoded, turn them  into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        # Normalise gradient\n",
    "        self.dinputs = self.dinputs/samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimisation Method - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimiser using Stochastic Gradient Descent, which assumes a batch of data. In this case it will work\n",
    "class Optimizer_SGD:\n",
    "    # Learning rate: stepsize\n",
    "    # decay: reducing factor of stepsize\n",
    "\n",
    "    def __init__(self, learning_rate, decay, momentum):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once, before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        # Checks if decay is not zero\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate*(1 + self.decay * self.iterations)\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        # if we decide to select a momentum that is not zero\n",
    "        if self.momentum:    \n",
    "            # If layer does not coontain momentum arrays, create them filled with zeros of same shape as layer, same for bias_momentum\n",
    "            if not hasattr(layer,'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            # Build weight update with momentum\n",
    "            weight_update = self.momentum*layer.weight_momentums - self.current_learning_rate*layer.dweights\n",
    "            layer.weight_momentums = weight_update\n",
    "            # Build bias update \n",
    "            bias_update = self.momentum*layer.bias_momentums - self.current_learning_rate*layer.dbiases\n",
    "            layer.bias_momentums = bias_update \n",
    "        # Vanilla SGD, without momentum\n",
    "        else: \n",
    "            weight_update = -self.current_learning_rate*layer.dweights\n",
    "            bias_update = -self.current_learning_rate*layer.dbiases\n",
    "        # Update weights and biases\n",
    "        layer.weights += weight_update\n",
    "        layer.biases += bias_update\n",
    "    \n",
    "    def post_update(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables to tinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "decay = 0\n",
    "momentum = 0\n",
    "hidden_neurons = 64\n",
    "n_input_params = 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,acc:0.013724137931034483,loss:2.058,lr:0.02\n",
      "epoch:100,acc:0.9209655172413793,loss:0.331,lr:0.02\n",
      "epoch:200,acc:0.9257701149425287,loss:0.279,lr:0.02\n",
      "epoch:300,acc:0.9164367816091954,loss:0.471,lr:0.02\n",
      "epoch:400,acc:0.9533563218390805,loss:0.170,lr:0.02\n",
      "epoch:500,acc:0.9637931034482758,loss:0.128,lr:0.02\n",
      "epoch:600,acc:0.9677011494252874,loss:0.106,lr:0.02\n",
      "epoch:700,acc:0.969816091954023,loss:0.090,lr:0.02\n",
      "epoch:800,acc:0.9765057471264368,loss:0.074,lr:0.02\n",
      "epoch:900,acc:0.9797011494252874,loss:0.065,lr:0.02\n",
      "epoch:1000,acc:0.9810344827586207,loss:0.060,lr:0.02\n",
      "epoch:1100,acc:0.9825747126436781,loss:0.052,lr:0.02\n",
      "epoch:1200,acc:0.9833103448275862,loss:0.050,lr:0.02\n",
      "epoch:1300,acc:0.9839080459770115,loss:0.047,lr:0.02\n",
      "epoch:1400,acc:0.984735632183908,loss:0.043,lr:0.02\n",
      "epoch:1500,acc:0.985264367816092,loss:0.039,lr:0.02\n",
      "epoch:1600,acc:0.9857471264367816,loss:0.037,lr:0.02\n",
      "epoch:1700,acc:0.986551724137931,loss:0.033,lr:0.02\n",
      "epoch:1800,acc:0.9874942528735632,loss:0.030,lr:0.02\n",
      "epoch:1900,acc:0.9868965517241379,loss:0.032,lr:0.02\n",
      "epoch:2000,acc:0.9943448275862069,loss:0.019,lr:0.02\n",
      "epoch:2100,acc:0.9966896551724138,loss:0.018,lr:0.02\n",
      "epoch:2200,acc:0.9970574712643678,loss:0.017,lr:0.02\n",
      "epoch:2300,acc:0.995816091954023,loss:0.016,lr:0.02\n",
      "epoch:2400,acc:0.9974022988505747,loss:0.014,lr:0.02\n",
      "epoch:2500,acc:0.9976781609195402,loss:0.013,lr:0.02\n",
      "epoch:2600,acc:0.994919540229885,loss:0.015,lr:0.02\n",
      "epoch:2700,acc:0.9857241379310345,loss:0.039,lr:0.02\n",
      "epoch:2800,acc:0.9930804597701149,loss:0.018,lr:0.02\n",
      "epoch:2900,acc:0.9974252873563219,loss:0.012,lr:0.02\n",
      "epoch:3000,acc:0.9975402298850574,loss:0.012,lr:0.02\n",
      "epoch:3100,acc:0.9976091954022989,loss:0.012,lr:0.02\n",
      "epoch:3200,acc:0.998183908045977,loss:0.011,lr:0.02\n",
      "epoch:3300,acc:0.9981149425287357,loss:0.009,lr:0.02\n",
      "epoch:3400,acc:0.9984137931034482,loss:0.009,lr:0.02\n",
      "epoch:3500,acc:0.9984137931034482,loss:0.009,lr:0.02\n",
      "epoch:3600,acc:0.9985287356321839,loss:0.008,lr:0.02\n",
      "epoch:3700,acc:0.9985977011494253,loss:0.008,lr:0.02\n",
      "epoch:3800,acc:0.9986896551724138,loss:0.007,lr:0.02\n",
      "epoch:3900,acc:0.9986896551724138,loss:0.007,lr:0.02\n",
      "epoch:4000,acc:0.9988275862068966,loss:0.007,lr:0.02\n",
      "epoch:4100,acc:0.998919540229885,loss:0.007,lr:0.02\n",
      "epoch:4200,acc:0.998919540229885,loss:0.006,lr:0.02\n",
      "epoch:4300,acc:0.9989425287356322,loss:0.006,lr:0.02\n",
      "epoch:4400,acc:0.998919540229885,loss:0.006,lr:0.02\n",
      "epoch:4500,acc:0.9990114942528736,loss:0.006,lr:0.02\n",
      "epoch:4600,acc:0.9990114942528736,loss:0.006,lr:0.02\n",
      "epoch:4700,acc:0.9990574712643678,loss:0.006,lr:0.02\n",
      "epoch:4800,acc:0.9991264367816092,loss:0.005,lr:0.02\n",
      "epoch:4900,acc:0.9991264367816092,loss:0.005,lr:0.02\n",
      "epoch:5000,acc:0.9991954022988506,loss:0.005,lr:0.02\n"
     ]
    }
   ],
   "source": [
    "# Open and read file\n",
    "trainingFile = open('shuttle.trn','r')\n",
    "lines = trainingFile.readlines()\n",
    "batch = np.zeros((len(lines),9))\n",
    "classfy = np.zeros(len(lines))\n",
    "\n",
    "# Set up input as a 2d array\n",
    "for i in range(len(lines)):\n",
    "    temp_str = lines[i].split(' ')\n",
    "    classfy[i] = int(temp_str[-1])\n",
    "    for j in range(9):\n",
    "        batch[i,j] = int(temp_str[j])\n",
    "        \n",
    "# Change category counting to start from 0 and convert to int\n",
    "classfy = np.asarray(classfy, dtype = 'int') - 1\n",
    "\n",
    "input_layer = Layer_FC(9,hidden_neurons)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer2 = Layer_FC(hidden_neurons,7)\n",
    "loss_activation = Activation_Softmax_Loss_CategoryCrossEntropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(learning_rate,decay,momentum)\n",
    "for epoch in range(5001):\n",
    "    # Forward pass, for activation function too\n",
    "    input_layer.forward(batch)\n",
    "    activation1.forward(input_layer.output)\n",
    "    # Forward pass on second layer, input is output of first layer (the activation output)\n",
    "    layer2.forward(activation1.output)\n",
    "    # Forward pass through activation/loss function \n",
    "    # Takes output of second layer and returns loss\n",
    "    loss = loss_activation.forward(layer2.output,classfy)\n",
    "    # Calculate the accuracy from output of activation2 and targets, along first axis\n",
    "    predictions = np.argmax(loss_activation.output,axis=1)\n",
    "    # if len(classfy.shape) == 2: **************\n",
    "\n",
    "    accuracy = np.mean(predictions==classfy)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch:{epoch},'+\n",
    "              f'acc:{accuracy},'+\n",
    "              f'loss:{loss:.3f},'+\n",
    "              f'lr:{optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output,classfy)\n",
    "    layer2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(layer2.dinputs)\n",
    "    input_layer.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(input_layer)\n",
    "    optimizer.update_params(layer2)\n",
    "    optimizer.post_update()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation,acc:0.998,loss:0.009\n"
     ]
    }
   ],
   "source": [
    "# Open and read file\n",
    "test_file = open('shuttle.tst','r')\n",
    "lines_tst = test_file.readlines()\n",
    "batch_tst = np.zeros((len(lines_tst),9))\n",
    "classfy_tst = np.zeros(len(lines_tst))\n",
    "\n",
    "# Set up input as a 2d array\n",
    "for i in range(len(lines_tst)):\n",
    "    temp_str = lines_tst[i].split(' ')\n",
    "    classfy_tst[i] = int(temp_str[-1])\n",
    "    for j in range(n_input_params):\n",
    "        batch_tst[i,j] = int(temp_str[j])\n",
    "\n",
    "# Change category counting to start from 0 and convert to int\n",
    "classfy_tst = np.asarray(classfy_tst, dtype = 'int') - 1\n",
    "# Forward pass\n",
    "input_layer.forward(batch_tst)\n",
    "activation1.forward(input_layer.output)\n",
    "layer2.forwardx(activation1.output)\n",
    "# Calculate loss\n",
    "loss = loss_activation.forward(layer2.output,classfy_tst)\n",
    "predictions = np.argmax(loss_activation.output,axis=1)\n",
    "accuracy = np.mean(predictions==classfy_tst)\n",
    "print(f'validation => acc:{accuracy:.3f},loss:{loss:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyEnviro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ab3354a82321b2ad6044469ba7b2eea47f41299af1b80d6b127938adffb245c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
